{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Simple Linear Regression?"
      ],
      "metadata": {
        "id": "nqtczyfkYZRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a basic statistical method used to model the relationship between **two variables**:\n",
        "\n",
        "* **Independent variable (X)** — the predictor/input\n",
        "* **Dependent variable (Y)** — the outcome/output you want to predict\n",
        "\n",
        "It assumes this relationship is **linear**, meaning Y changes at a constant rate with X.\n",
        "\n",
        "---\n",
        "\n",
        "### **The Equation**\n",
        "\n",
        "The model is written as:\n",
        "\n",
        "[\n",
        "Y = b_0 + b_1X\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* **(b_0)** = intercept (value of Y when X = 0)\n",
        "* **(b_1)** = slope (how much Y changes when X increases by 1 unit)\n",
        "\n",
        "---\n",
        "\n",
        "### **Goal of Simple Linear Regression**\n",
        "\n",
        "To find the *best-fitting* straight line through the data points by minimizing the error between predicted and actual values. The method used is **Least Squares**, which minimizes:\n",
        "\n",
        "[\n",
        "\\sum (Y_{actual} - Y_{predicted})^2\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "### **When Is It Used?**\n",
        "\n",
        "* Predicting house price from size\n",
        "* Predicting sales from advertising spend\n",
        "* Predicting weight from height\n",
        "* Any case where one variable depends on another linearly\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation Example**\n",
        "\n",
        "If your regression gives:\n",
        "\n",
        "[\n",
        "Y = 2 + 3X\n",
        "]\n",
        "\n",
        "Then:\n",
        "\n",
        "* When X increases by 1, Y increases by 3\n",
        "* When X = 0, Y = 2\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also show:\n",
        "\n",
        "* A simple graph\n",
        "* How to compute regression by hand\n",
        "* How to do it in Python, Excel, or R\n",
        "  Which one would you like?\n"
      ],
      "metadata": {
        "id": "qb6v5DyUYkgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "4GOTCIRDYptw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression relies on several key assumptions to ensure the model’s predictions and statistical tests are valid. Here are the main ones:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Linearity**\n",
        "\n",
        "The relationship between the independent variable (X) and the dependent variable (Y) is **linear**.\n",
        "\n",
        "[\n",
        "Y = b_0 + b_1X + \\epsilon\n",
        "]\n",
        "\n",
        "This means changes in X correspond to proportional changes in Y.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Independence of Errors**\n",
        "\n",
        "The residuals (errors) must be **independent** of each other.\n",
        "Violations usually occur in **time-series data**, where errors may correlate over time (autocorrelation).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Homoscedasticity (Constant Variance of Errors)**\n",
        "\n",
        "The residuals should have **constant variance** across all levels of X.\n",
        "If variance increases or decreases with X (heteroscedasticity), predictions become unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Normality of Errors**\n",
        "\n",
        "The residuals (not the variables!) should be **normally distributed**.\n",
        "This matters mainly for hypothesis testing and constructing confidence intervals.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. No or Minimal Multicollinearity**\n",
        "\n",
        "*(Only relevant in multiple regression; not an issue for simple regression because there is only one predictor.)*\n",
        "\n",
        "---\n",
        "\n",
        "## **6. No Measurement Error in X**\n",
        "\n",
        "The independent variable should be measured without significant error.\n",
        "If X has measurement errors, the slope becomes biased.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Observations Are Randomly Sampled**\n",
        "\n",
        "Data should be collected from the population in a random manner to avoid bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Assumption                | Meaning                           | Consequence if violated         |\n",
        "| ------------------------- | --------------------------------- | ------------------------------- |\n",
        "| Linearity                 | X and Y relate linearly           | Poor fit, biased predictions    |\n",
        "| Independence              | Residuals unrelated               | Invalid significance tests      |\n",
        "| Homoscedasticity          | Constant error variance           | Inefficient & biased estimates  |\n",
        "| Normality                 | Errors follow normal distribution | Confidence intervals unreliable |\n",
        "| No measurement error in X | Predictor is accurate             | Biased slope                    |\n",
        "| Random sampling           | Observations representative       | Model cannot generalize         |\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also explain:\n",
        "\n",
        "* How to **check** these assumptions\n",
        "* How to **fix** violations (transformations, residual plots, etc.)\n",
        "  Just let me know!\n"
      ],
      "metadata": {
        "id": "8Ll3EGAqY3Gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.: What is heteroscedasticity, and why is it important to address in regression models?"
      ],
      "metadata": {
        "id": "h2gZTZviY8BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heteroscedasticity** refers to a condition in regression analysis where the **variance of the residuals (errors) is not constant** across all levels of the independent variable(s).\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "> The spread of errors changes as X changes.\n",
        "> (e.g., errors get larger when X increases)\n",
        "\n",
        "---\n",
        "\n",
        "## **Example (Visual Intuition)**\n",
        "\n",
        "If you plot residuals vs. predicted values, heteroscedasticity looks like:\n",
        "\n",
        "* A funnel shape\n",
        "* A cone shape\n",
        "* Increasing or decreasing spread of residuals\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Is Heteroscedasticity Important to Address?**\n",
        "\n",
        "### **1. It Violates a Key Regression Assumption**\n",
        "\n",
        "Regression assumes **homoscedasticity**, meaning constant variance of errors.\n",
        "Violation leads to unreliable inference.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Standard Errors Become Biased**\n",
        "\n",
        "When residual variance is not constant:\n",
        "\n",
        "* Estimated **standard errors** of coefficients become incorrect\n",
        "* This affects:\n",
        "\n",
        "  * t-tests\n",
        "  * F-tests\n",
        "  * p-values\n",
        "  * Confidence intervals\n",
        "\n",
        "You may incorrectly conclude that a predictor is significant when it is not — or vice versa.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Coefficient Estimates Are Still Unbiased**\n",
        "\n",
        "OLS still gives **unbiased** regression coefficients even with heteroscedasticity.\n",
        "However, the coefficients are **no longer efficient** (not minimum variance).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Predictions Become Less Reliable**\n",
        "\n",
        "The model may perform poorly on out-of-sample data because it doesn't capture the true error structure.\n",
        "\n",
        "---\n",
        "\n",
        "## **How to Detect It?**\n",
        "\n",
        "* Residuals vs. fitted values plot\n",
        "* Breusch–Pagan test\n",
        "* White’s test\n",
        "* Goldfeld–Quandt test\n",
        "\n",
        "---\n",
        "\n",
        "## **How to Fix It?**\n",
        "\n",
        "* Log or square-root transformations\n",
        "* Weighted Least Squares (WLS)\n",
        "* Using robust standard errors (e.g., HAC, White’s robust SEs)\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary**\n",
        "\n",
        "**Heteroscedasticity = non-constant error variance.**\n",
        "\n",
        "It must be addressed because it leads to:\n",
        "\n",
        "* Biased standard errors\n",
        "* Incorrect hypothesis testing\n",
        "* Inefficient estimates\n",
        "* Lower predictive accuracy\n",
        "\n",
        "If you want, I can also show you **a simple graph**, **how to detect heteroscedasticity in Python**, or **exam-ready short answers**.\n"
      ],
      "metadata": {
        "id": "JDOkyaexZGKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. : What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "jymkaBOiZNd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Linear Regression (MLR)** is an extension of simple linear regression that models the relationship between **one dependent variable (Y)** and **two or more independent variables (X₁, X₂, X₃, …)**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Definition**\n",
        "\n",
        "Multiple Linear Regression estimates how several predictors collectively influence an outcome.\n",
        "\n",
        "The model is:\n",
        "\n",
        "[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\cdots + b_kX_k + \\epsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* **Y** = dependent (response) variable\n",
        "* **X₁, X₂, …, X_k** = independent (predictor) variables\n",
        "* **b₀** = intercept\n",
        "* **b₁, b₂, …, b_k** = regression coefficients (effect of each predictor)\n",
        "* **ε** = error term\n",
        "\n",
        "---\n",
        "\n",
        "## **Purpose**\n",
        "\n",
        "Multiple Linear Regression helps to:\n",
        "\n",
        "* Predict an outcome from several factors\n",
        "* Understand how each predictor influences the dependent variable\n",
        "* Control for the effects of other variables\n",
        "* Identify which variables are significant\n",
        "\n",
        "---\n",
        "\n",
        "## **Example**\n",
        "\n",
        "Predicting **house price (Y)** using:\n",
        "\n",
        "* Size of house (X1)\n",
        "* Number of rooms (X2)\n",
        "* Age of house (X3)\n",
        "\n",
        "Model:\n",
        "\n",
        "[\n",
        "\\text{Price} = b_0 + b_1(\\text{Size}) + b_2(\\text{Rooms}) + b_3(\\text{Age})\n",
        "]\n",
        "\n",
        "Each coefficient shows how much the price changes when one variable changes, **holding others constant**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Advantages**\n",
        "\n",
        "* More accurate predictions than simple linear regression\n",
        "* Can analyze complex real-world relationships\n",
        "* Allows control of confounding variables\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Assumptions**\n",
        "\n",
        "* Linearity\n",
        "* Independence of errors\n",
        "* Homoscedasticity\n",
        "* Normality of residuals\n",
        "* **No multicollinearity** (predictors should not be highly correlated)\n",
        "\n",
        "---\n",
        "\n",
        "If you'd like, I can also explain:\n",
        "\n",
        "* Difference between simple and multiple regression\n",
        "* How to interpret coefficients\n",
        "* An example with real numbers\n",
        "* Python / Excel implementation\n"
      ],
      "metadata": {
        "id": "pL06We51ZWde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.: What is polynomial regression, and how does it differ from linear regression?"
      ],
      "metadata": {
        "id": "Z2bnoaufZc49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Polynomial Regression** is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an **nth-degree polynomial**.\n",
        "\n",
        "---\n",
        "\n",
        "# **Definition**\n",
        "\n",
        "Polynomial regression fits a curve to the data:\n",
        "\n",
        "[\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\cdots + b_nX^n + \\epsilon\n",
        "]\n",
        "\n",
        "It is useful when the relationship between X and Y is **nonlinear**, but can still be represented by a smooth curve.\n",
        "\n",
        "---\n",
        "\n",
        "# **How It Differs from Linear Regression**\n",
        "\n",
        "### **1. Shape of the Relationship**\n",
        "\n",
        "* **Linear Regression:** Fits a straight line\n",
        "  [\n",
        "  Y = b_0 + b_1X\n",
        "  ]\n",
        "* **Polynomial Regression:** Fits a curved line (quadratic, cubic, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Number of Terms**\n",
        "\n",
        "* Simple linear regression uses only **X**.\n",
        "* Polynomial regression uses **X, X², X³, …** up to degree *n*.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Flexibility**\n",
        "\n",
        "* Polynomial regression is more flexible and can model complex relationships.\n",
        "* Linear regression is more restrictive (straight-line relation only).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Still Linear in Parameters**\n",
        "\n",
        "Even though the model forms a **curved line**, polynomial regression is still considered a **linear model** because the coefficients (b₀, b₁, b₂…) appear linearly.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Risk of Overfitting**\n",
        "\n",
        "* Higher-degree polynomials can fit the training data very closely.\n",
        "* But may perform poorly on new data.\n",
        "\n",
        "---\n",
        "\n",
        "# **Example Comparison**\n",
        "\n",
        "### **Linear Regression Example:**\n",
        "\n",
        "[\n",
        "Y = 3 + 2X\n",
        "]\n",
        "→ A straight line.\n",
        "\n",
        "### **Polynomial Regression Example (Quadratic):**\n",
        "\n",
        "[\n",
        "Y = 3 + 2X + 0.5X^2\n",
        "]\n",
        "→ A curved parabola.\n",
        "\n",
        "---\n",
        "\n",
        "# **When to Use Polynomial Regression**\n",
        "\n",
        "Use it when:\n",
        "\n",
        "* The relationship between variables is curved\n",
        "* Linear regression leaves systematic patterns in residuals\n",
        "* You want a simple way to model nonlinearity\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also explain:\n",
        "\n",
        "* When polynomial regression becomes too complex\n",
        "* How to choose the degree of the polynomial\n",
        "* Polynomial regression in Python or Excel\n"
      ],
      "metadata": {
        "id": "HQMl8PAIZnD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Implement a Python program to fit a Simple Linear Regression model to the following sample data: ● X = [1, 2, 3, 4, 5] ● Y = [2.1, 4.3, 6.1, 7.9, 10.2] Plot the regression line over the data points. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "pagwV9WwZvpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is your **Simple Linear Regression** plot and the fitted model.\n",
        "\n",
        "The regression coefficients calculated are:\n",
        "\n",
        "* **Slope (b₁)** ≈ **1.98**\n",
        "* **Intercept (b₀)** ≈ **0.18**\n",
        "\n",
        "Your scatter points and regression line are shown in the plot above.\n"
      ],
      "metadata": {
        "id": "CLii7jVNaXMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Fit a Multiple Linear Regression model on this sample data: ● Area = [1200, 1500, 1800, 2000] ● Rooms = [2, 3, 3, 4] ● Price = [250000, 300000, 320000, 370000] Check for multicollinearity using VIF and report the results. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "KYb5y1uFakD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is your Multiple Linear Regression model and multicollinearity (VIF) check.\n",
        "The code block above shows:\n",
        "\n",
        "### **Regression Inputs**\n",
        "\n",
        "* **Area**, **Rooms** → Predict **Price**\n",
        "\n",
        "### **VIF Results**\n",
        "\n",
        "| Feature | VIF   |\n",
        "| ------- | ----- |\n",
        "| const   | 34.21 |\n",
        "| Area    | 7.74  |\n",
        "| Rooms   | 7.74  |\n",
        "\n",
        "### **Interpretation**\n",
        "\n",
        "* VIF > 5 indicates **moderate multicollinearity**\n",
        "* VIF > 10 indicates **serious multicollinearity**\n",
        "\n",
        "Here, both **Area** and **Rooms** have VIF ≈ 7.7 →\n",
        "➡️ **Moderate multicollinearity**\n",
        "➡️ The predictors are correlated (which makes sense: larger houses tend to have more rooms)\n",
        "\n",
        "If you want, I can also provide:\n",
        "\n",
        "* The regression equation\n",
        "* Coefficient values\n",
        "* Predictions\n",
        "* A correlation heatmap\n",
        "  Just tell me!\n"
      ],
      "metadata": {
        "id": "QntAHIlEbctr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Implement polynomial regression on the following data: ● X = [1, 2, 3, 4, 5] 3 ● Y = [2.2, 4.8, 7.5, 11.2, 14.7] Fit a 2nd-degree polynomial and plot the resulting curve. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "qF-OQsslbf4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your 2nd-degree polynomial regression has been successfully fitted and plotted!\n",
        "The curve captures the nonlinear growth pattern in the data nicely.\n",
        "\n",
        "If you want, I can also provide:\n",
        "\n",
        "* The polynomial equation\n",
        "* Coefficient values\n",
        "* Prediction for any new X\n",
        "* Comparison with linear regression\n",
        "\n",
        "Just let me know!\n"
      ],
      "metadata": {
        "id": "eeauxgkhbrDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Create a residuals plot for a regression model trained on this data: ● X = [10, 20, 30, 40, 50] ● Y = [15, 35, 40, 50, 65] Assess heteroscedasticity by examining the spread of residuals. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "laMKRjDTcQMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your residuals plot has been successfully generated.\n",
        "\n",
        "### **Heteroscedasticity Assessment**\n",
        "\n",
        "By observing the spread of the residuals:\n",
        "\n",
        "* The residuals are **not evenly distributed** across X.\n",
        "* At X = 20, the residual is high (positive).\n",
        "* At X = 10 and 40, the residuals are strongly negative.\n",
        "* The spread varies as X increases.\n",
        "\n",
        "➡️ **This indicates mild heteroscedasticity** — the variance of residuals changes with X.\n",
        "\n",
        "If you want, I can also:\n",
        "\n",
        "* Fit the model using scikit-learn\n",
        "* Provide numerical residuals\n",
        "* Perform a statistical test for heteroscedasticity (Breusch–Pagan)\n"
      ],
      "metadata": {
        "id": "5wWYfx9KchVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.magine you are a data scientist working for a real estate company. You need to predict house prices using features like area, number of rooms, and location. However, you detect heteroscedasticity and multicollinearity in your regression model. Explain the steps you would take to address these issues and ensure a robust model. Answer:  "
      ],
      "metadata": {
        "id": "AU5wVmogcp23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a clear and structured explanation you can use as your answer:\n",
        "\n",
        "---\n",
        "\n",
        "### **Answer:**\n",
        "\n",
        "As a data scientist predicting house prices, detecting **heteroscedasticity** and **multicollinearity** indicates that the regression model may produce unreliable estimates. To ensure a robust and accurate model, I would take the following steps:\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **1. Addressing Heteroscedasticity**\n",
        "\n",
        "Heteroscedasticity means the variance of residuals changes with the level of predictors. This violates OLS assumptions and leads to inefficient estimates.\n",
        "\n",
        "### **Steps to fix it:**\n",
        "\n",
        "### **a. Apply a log or Box-Cox transformation**\n",
        "\n",
        "* Transform the target variable:\n",
        "\n",
        "  * ( y' = \\log(y) )\n",
        "* This stabilizes variance and makes residuals more uniform.\n",
        "\n",
        "### **b. Use Weighted Least Squares (WLS)**\n",
        "\n",
        "* Assign lower weights to points with higher variance.\n",
        "* Helps when variance clearly increases with predictors like *area*.\n",
        "\n",
        "### **c. Add or transform features**\n",
        "\n",
        "* If heteroscedasticity comes from nonlinearity:\n",
        "\n",
        "  * Add polynomial terms (e.g., area²)\n",
        "  * Bin locations into categories\n",
        "\n",
        "### **d. Use robust standard errors**\n",
        "\n",
        "* HAC (White) standard errors fix confidence intervals without changing prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **2. Addressing Multicollinearity**\n",
        "\n",
        "Multicollinearity occurs when predictors—such as *area*, *number of rooms*, and *location rating*—are highly correlated. This inflates the variance of coefficients.\n",
        "\n",
        "### **Steps to fix it:**\n",
        "\n",
        "### **a. Remove or combine correlated features**\n",
        "\n",
        "* If *area* and *number of rooms* are strongly correlated:\n",
        "\n",
        "  * Remove one\n",
        "  * Or combine into *area per room*\n",
        "\n",
        "### **b. Use dimensionality reduction**\n",
        "\n",
        "* Apply **Principal Component Analysis (PCA)** to create independent components from correlated variables.\n",
        "\n",
        "### **c. Apply Regularization (Ridge or Lasso)**\n",
        "\n",
        "* **Ridge:** reduces coefficient variance\n",
        "* **Lasso:** can remove unnecessary variables\n",
        "* Both reduce impacts of multicollinearity and increase stability.\n",
        "\n",
        "### **d. Check VIF (Variance Inflation Factor)**\n",
        "\n",
        "* Remove features with **VIF > 10**, or investigate correlation patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **3. Rebuild & Validate the Model**\n",
        "\n",
        "After corrections:\n",
        "\n",
        "### **a. Refit the regression model**\n",
        "\n",
        "* Using transformed or regularized features.\n",
        "\n",
        "### **b. Recheck residual diagnostics**\n",
        "\n",
        "* Plot residuals vs. fitted values\n",
        "* Ensure variance is stable\n",
        "\n",
        "### **c. Validate using cross-validation**\n",
        "\n",
        "* Ensures improvements generalize to unseen data.\n",
        "\n",
        "### **d. Compare performance**\n",
        "\n",
        "* Evaluate metrics like RMSE, MAE, R² before and after corrective steps.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Result**\n",
        "\n",
        "By applying transformations, robust methods, and regularization, the regression model becomes more stable, reliable, and accurate—leading to better house-price predictions and improved business decisions.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also convert this into a shorter exam-style answer or a paragraph-form answer.\n"
      ],
      "metadata": {
        "id": "MGW6dNhGcr2q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}